{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manikanta898/Unsupervised-ML---Netflix-Movies-and-TV-Shows-Clustering/blob/main/Netflix_Movies_and_TV_Shows_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vncDsAP0Gaoa"
      },
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beRrZCGUAJYm"
      },
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member**    - Manikanta Tangi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJNUwmbgGyua"
      },
      "source": [
        "# **Project Summary -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6v_1wHtG2nS"
      },
      "source": [
        "\n",
        "In this unsupervised machine learning project, the objective is to develop a model capable of clustering diverse types of data from a dataset of Netflix shows. The dataset consists of 7787 entries distributed across 12 columns, including show_id (show identifier), type (movie or TV show), title, cast, country (show's origin), date added (to Netflix), release_year, rating, duration (show length), listed_in (genre categorization), and description (show summary).This project involves exploring and visualizing a dataset of Netflix shows to identify trends and patterns. Hypotheses will be formulated based on these insights and tested for validation. Data preprocessing tasks will address missing values, outliers, and imbalanced data to ensure dataset quality. Finally, the dataset will be split into training and testing sets to build and evaluate clustering models aimed at categorizing Netflix shows into meaningful clusters based on their attributes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6K7xa23Elo4"
      },
      "source": [
        "# **GitHub Link -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1o69JH3Eqqn"
      },
      "source": [
        "https://github.com/mani-github2021/Unsupervised-ML---Netflix-Movies-and-TV-Shows-Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQaldy8SH6Dl"
      },
      "source": [
        "# **Problem Statement**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpeJGUA3kjGy"
      },
      "source": [
        "The goal of this project is to explore and analyze the dataset of TV shows and movies available on Netflix as of 2019. The analysis aims to uncover insights about the content available on Netflix, trends in TV shows and movies, and other patterns that can inform business decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_i_v8NEhb9l"
      },
      "source": [
        "# ***Let's Begin !***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhfV-JJviCcP"
      },
      "source": [
        "## ***1. Know Your Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3lxredqlCYt"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RnN4peoiCZX"
      },
      "source": [
        "### Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "path='/content/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv'\n",
        "df = pd.read_csv(path, on_bad_lines='skip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x71ZqKXriCWQ"
      },
      "source": [
        "### Dataset First View"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "outputs": [],
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hBIi_osiCS2"
      },
      "source": [
        "### Dataset Rows & Columns count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "outputs": [],
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlHwYmJAmNHm"
      },
      "source": [
        "### Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "outputs": [],
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35m5QtbWiB9F"
      },
      "source": [
        "#### Duplicate Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "outputs": [],
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoPl-ycgm1ru"
      },
      "source": [
        "#### Missing Values/Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "outputs": [],
      "source": [
        "# Calculate the count of missing values for each column\n",
        "missing_count = df.isnull().sum()\n",
        "\n",
        "# Calculate the percentage of missing values for each column\n",
        "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
        "\n",
        "# Combine both into a DataFrame\n",
        "missing_info = pd.DataFrame({\n",
        "    'Missing Values Count': missing_count,\n",
        "    'Missing Values Percentage': missing_percentage\n",
        "})\n",
        "\n",
        "# Display the result\n",
        "print(missing_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "outputs": [],
      "source": [
        "# Visualizing the missing values\n",
        "plt.rcParams['figure.figsize'] = (10,5)\n",
        "df.isna().sum().plot.bar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0kj-8xxnORC"
      },
      "source": [
        "### What did you know about your dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfoNAAC-nUe_"
      },
      "source": [
        "This dataset consists of 7787 rows and 12 columns in which director, cast, country, date_added and rating columns are having missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      },
      "source": [
        "## ***2. Understanding Your Variables***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "outputs": [],
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "outputs": [],
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBTbrJXOngz2"
      },
      "source": [
        "### Variables Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJV4KIxSnxay"
      },
      "source": [
        "These are the variables in the dataset:\n",
        "\n",
        "Show_id: Show ID\n",
        "\n",
        "Type: Type of the show (e.g., movie or TV show)\n",
        "\n",
        "Title: Title of the show\n",
        "\n",
        "Director: Director of the show\n",
        "\n",
        "Cast: Actors and actresses of the show\n",
        "\n",
        "Country: Country of origin of the show\n",
        "\n",
        "Date_added: Date when the show was added to Netflix\n",
        "\n",
        "Release_year: Release year\n",
        "\n",
        "Rating: Rating of the show\n",
        "\n",
        "Duration: Duration of the show\n",
        "\n",
        "Listed_in: Categories the show is listed in\n",
        "\n",
        "Description: Description of the show"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3PMJOP6ngxN"
      },
      "source": [
        "### Check Unique Values for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "outputs": [],
      "source": [
        "# Check Unique Values for each variable.\n",
        "for col in df.columns:\n",
        "    print(f\"Number of unique value for {col} is : {len(df[col].unique())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dauF4eBmngu3"
      },
      "source": [
        "## 3. ***Data Wrangling***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJF3rekwFvQ"
      },
      "source": [
        "### Data Wrangling Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "outputs": [],
      "source": [
        "# Handle missing values\n",
        "df['director'].fillna('Unknown', inplace=True)\n",
        "df['cast'].fillna('Unknown', inplace=True)\n",
        "df['country'].fillna('Unknown', inplace=True)\n",
        "df['date_added'].fillna('Unknown', inplace=True)\n",
        "df['rating'].fillna('Unrated', inplace=True)\n",
        "\n",
        "# Convert date_added to datetime\n",
        "df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')\n",
        "\n",
        "# Extract month and year from date_added\n",
        "df['month_added'] = df['date_added'].dt.month\n",
        "df['year_added'] = df['date_added'].dt.year\n",
        "\n",
        "# Separate duration into minutes and seasons\n",
        "df['duration_num'] = df['duration'].str.extract('(\\d+)').astype(float)\n",
        "df.loc[df['type'] == 'TV Show', 'duration_type'] = 'Seasons'\n",
        "df.loc[df['type'] == 'Movie', 'duration_type'] = 'Minutes'\n",
        "\n",
        "# Check the updated dataframe\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Butms8rSOt9I"
      },
      "outputs": [],
      "source": [
        "# Calculate total length for each type of duration (Movies and TV Shows)\n",
        "total_duration = df.groupby('duration_type')['duration_num'].sum()\n",
        "total_duration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M01dd8YhOupX"
      },
      "outputs": [],
      "source": [
        "# Calculate total number of shows added for each month\n",
        "total_shows_per_month = df.groupby('month_added')['show_id'].count()\n",
        "total_shows_per_month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Gf86rd4Ou-2"
      },
      "outputs": [],
      "source": [
        "# Calculate total number of movies added for each month\n",
        "total_movies_per_month = df[df['type'] == 'Movie'].groupby('month_added')['show_id'].count()\n",
        "total_movies_per_month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVt77bjZOvQe"
      },
      "outputs": [],
      "source": [
        "# Calculate total number of TV shows added for each month\n",
        "total_tvshows_per_month = df[df['type'] == 'TV Show'].groupby('month_added')['show_id'].count()\n",
        "total_tvshows_per_month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QG_xnBk2OviM"
      },
      "outputs": [],
      "source": [
        "# Assign top 5 actors to a variable\n",
        "top_5_actors = df['cast'].str.split(', ').explode().value_counts().head(5)\n",
        "# Assign top 5 genres to a variable\n",
        "top_5_genres = df['listed_in'].str.split(', ').explode().value_counts().head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XePjRN1AOwJ2"
      },
      "outputs": [],
      "source": [
        "# Top 5 actors with the highest number of shows\n",
        "top_actors = df['cast'].str.split(', ').explode().value_counts().head(5)\n",
        "top_actors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVujVDlyOwc3"
      },
      "outputs": [],
      "source": [
        "# Top 5 directors with the highest number of shows\n",
        "top_directors = df['director'].value_counts().head(5)\n",
        "top_directors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rxXw8DPOwzJ"
      },
      "outputs": [],
      "source": [
        "# Top 5 countries with the highest number of shows\n",
        "top_countries = df['country'].value_counts().head(5)\n",
        "top_countries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttk4J33ROxMm"
      },
      "outputs": [],
      "source": [
        "# Top 5 years with the highest number of shows released\n",
        "top_years = df['release_year'].value_counts().head(5)\n",
        "top_years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTqEphUlOxe_"
      },
      "outputs": [],
      "source": [
        "# Top ratings with the highest number of shows\n",
        "top_ratings = df['rating'].value_counts().head(5)\n",
        "top_ratings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSa1f5Uengrz"
      },
      "source": [
        "### What all manipulations have you done and insights you found?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbyXE7I1olp8"
      },
      "source": [
        "Handled Missing Values:\n",
        "\n",
        "Filled missing values in director, cast, country, date_added, and rating with appropriate placeholders.\n",
        "Converted Data Types:\n",
        "\n",
        "Converted date_added to datetime format.\n",
        "Extracted Additional Features:\n",
        "\n",
        "Extracted month_added and year_added from date_added.\n",
        "Separated duration into duration_num and added a new column duration_type to distinguish between minutes and seasons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k69u9aFBQDvq"
      },
      "source": [
        "Directors with highest number of movies/tv shows are Raúl Campos and Jan Suter : 18\n",
        "\n",
        "Top 5 countries produced highest number of movies are :\n",
        "\n",
        "United States : 2555 India : 923 United Kingdom : 397 Japan : 226 South Korea : 183\n",
        "\n",
        "Highest movies produced in year 2018 : 1121\n",
        "\n",
        "Rating for highest number of movie is TV-MA : 2863\n",
        "\n",
        "Top 5 actors with highest number of movies are :\n",
        "\n",
        "Anupam Kher : 42 Shah Rukh Khan : 35 Naseeruddin Shah : 30 Om Puri : 30 Akshay Kumar : 29\n",
        "\n",
        "Top genres with highest number of movies are :\n",
        "International Movies : 2437 Dramas : 2106\n",
        "\n",
        "Total Seasons : 4280 and Total Minutes : 533979.\n",
        "\n",
        "Number of TV Shows/Movies for each month\n",
        "\n",
        "December : 833\n",
        "\n",
        "October : 785\n",
        "\n",
        "January : 757\n",
        "\n",
        "November : 738\n",
        "\n",
        "March : 669\n",
        "\n",
        "September : 619\n",
        "\n",
        "August : 618\n",
        "\n",
        "April : 601\n",
        "\n",
        "July : 600\n",
        "\n",
        "May : 543\n",
        "\n",
        "June : 542\n",
        "\n",
        "February : 472"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF8Ens_Soomf"
      },
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wOQAZs5pc--"
      },
      "source": [
        "#### Chart - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "outputs": [],
      "source": [
        "# Chart - 1 : visualization code\n",
        "type_counts = df['type'].value_counts()\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.pie(type_counts, labels=type_counts.index, autopct='%1.1f%%', colors=['#66b3ff', '#99ff99'], startangle=140)\n",
        "plt.title('Distribution of TV Shows vs. Movies')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5QZ13OEpz2H"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XESiWehPqBRc"
      },
      "source": [
        "Pie charts are used to visually represent the proportion of different categories within a whole.This chart shows percentage of TV shows and movies the in dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_j1G7yiqdRP"
      },
      "source": [
        "69% share for TV shows shows that people are watching more series than movies, likely because of the rise of streaming platforms and a preference for longer, more engaging content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "448CDAPjqfQr"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSlN3yHqYklG"
      },
      "source": [
        "This insight can help Netflix understand its content composition, enabling better content strategy and investment decisions. If there is a significant imbalance, it might indicate the need for more TV shows or movies to attract and retain subscribers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsA7UlComYj6"
      },
      "source": [
        "#### Chart - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "outputs": [],
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.countplot(data=df, x='year_added', palette='coolwarm')\n",
        "plt.title('Year-wise Addition of Content on Netflix')\n",
        "plt.xlabel('Year Added')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6dVpIINYklI"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aaW0BYyYklI"
      },
      "source": [
        "A count plot visualizes the number of occurrences of each category in a dataset. This chart shows how much data is added year wisely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijmpgYnKYklI"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSx9atu2YklI"
      },
      "source": [
        "I found, in the year 2019, maximum of content added on netflix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JiQyfWJYklI"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcBbebzrYklV"
      },
      "source": [
        "The chart reveals which years saw the most content added, indicating periods of rapid content growth.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM7whBJCYoAo"
      },
      "source": [
        "#### Chart - 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "outputs": [],
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.countplot(data=df, x='rating', palette='muted', order=df['rating'].value_counts().index)\n",
        "plt.title('Distribution of Content Ratings on Netflix')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fge-S5ZAYoAp"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dBItgRVYoAp"
      },
      "source": [
        "A count plot visualizes the number of occurrences of each category in a dataset. This chart shows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85gYPyotYoAp"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jstXR6OYoAp"
      },
      "source": [
        "The chart shows that Netflix primarily targets mature audiences with a high proportion of TV-MA content, while NC-17 content is rare."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoGjAbkUYoAp"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      },
      "source": [
        "This strategy may boost retention among mature viewers but could limit appeal to younger or more conservative audiences, potentially restricting growth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Of9eVA-YrdM"
      },
      "source": [
        "#### Chart - 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "outputs": [],
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(12,8))\n",
        "top_genres = df['listed_in'].str.split(', ', expand=True).stack().value_counts().head(10)\n",
        "top_genres.plot(kind='bar', color='skyblue')\n",
        "plt.title('Top 10 Genres on Netflix')\n",
        "plt.xlabel('Genre')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iky9q4vBYrdO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJRCwT6DYrdO"
      },
      "source": [
        "A bar chart compares quantities across categories for easy visual analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6T5p64dYrdO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      },
      "source": [
        "The chart shows that International Movies have the highest count, while Romantic Movies are the least represented on Netflix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-Ehk30pYrdP"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLNxxz7MYrdP"
      },
      "source": [
        "Focusing on International Movies could attract a global audience, increasing subscriptions, while the low representation of Romantic Movies may limit appeal to fans of that genre, affecting potential growth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bamQiAODYuh1"
      },
      "source": [
        "#### Chart - 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "outputs": [],
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.figure(figsize=(14,10))\n",
        "top_countries = df['country'].value_counts().head(10)\n",
        "sns.barplot(y=top_countries.index, x=top_countries.values, palette='deep')\n",
        "plt.title('Top 10 Countries by Number of Titles on Netflix')\n",
        "plt.xlabel('Number of Titles')\n",
        "plt.ylabel('Country')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcxuIMRPYuh3"
      },
      "source": [
        "A horizontal bar plot displays categories with horizontal bars, making comparisons easy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwzvFGzlYuh3"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyqkiB8YYuh3"
      },
      "source": [
        "The United States has the most titles, while Egypt has the fewest on Netflix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYpmQ266Yuh3"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      },
      "source": [
        "More titles in the US can drive subscriptions, but fewer titles in Egypt may limit growth in that market."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH-pJp9IphqM"
      },
      "source": [
        "#### Chart - 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "outputs": [],
      "source": [
        "# Chart - 6 visualization code\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.countplot(data=df, x='month_added', palette='Set2')\n",
        "plt.title('Monthly Content Addition Trend on Netflix')\n",
        "plt.xlabel('Month Added')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbFf2-_FphqN"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loh7H2nzphqN"
      },
      "source": [
        "A count plot visualizes the number of occurrences of each category in a dataset. This chart shows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ouA3fa0phqN"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VECbqPI7phqN"
      },
      "source": [
        "December sees the highest content addition, while February has the lowest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Seke61FWphqN"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW4_bGpfphqN"
      },
      "source": [
        "High December content boosts engagement, but February's low addition may cause churn if unaddressed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIIx-8_IphqN"
      },
      "source": [
        "#### Chart - 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "outputs": [],
      "source": [
        "# Chart - 7 visualization code\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.histplot(data=df, x='duration_num', bins=30, kde=True)\n",
        "plt.title('Distribution of Content Duration on Netflix')\n",
        "plt.xlabel('Duration (Minutes/Seasons)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t27r6nlMphqO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv6ro40sphqO"
      },
      "source": [
        "A histplot shows the distribution of data by displaying frequencies of values within specified bins."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2jJGEOYphqO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po6ZPi4hphqO"
      },
      "source": [
        "Majority content is short (0-100 mins) and very few long-format shows (>150 mins)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0JNsNcRphqO"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvSq8iUTphqO"
      },
      "source": [
        "The content is positive and varied, catering to different viewer preferences. However, having a lot of short videos may slightly affect the total watch time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZR9WyysphqO"
      },
      "source": [
        "#### Chart - 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "outputs": [],
      "source": [
        "# Chart - 8 visualization code\n",
        "rating_counts = df['rating'].value_counts()\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.barplot(x=rating_counts.index, y=rating_counts.values, palette=\"pastel\")\n",
        "plt.title('Distribution of Content by Ratings')\n",
        "plt.xlabel('Ratings')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj7wYXLtphqO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob8u6rCTphqO"
      },
      "source": [
        "A bar chart compares quantities across categories for easy visual analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZrbJ2SmphqO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZtgC_hjphqO"
      },
      "source": [
        "Most shows are for adults (TV-MA) and teens (TV-14), with very few kids' shows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFu4xreNphqO"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey_0qi68phqO"
      },
      "source": [
        "Adult content selection attracts mature audiences but limited family/children content could restrict subscriber growth across demographics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC_X3p0fY2L0"
      },
      "source": [
        "#### Chart - 9 - Correlation Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iq0ITt_fk1-j"
      },
      "outputs": [],
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# Converting date_added to datetime\n",
        "df['date_added'] = pd.to_datetime(df['date_added'])\n",
        "\n",
        "# Selecting only numeric columns for correlation analysis\n",
        "numeric_df = df.select_dtypes(include=['number'])\n",
        "\n",
        "# Creating a correlation matrix for numeric columns\n",
        "corr_matrix = numeric_df.corr()\n",
        "\n",
        "# Plotting the heatmap\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"YlGnBu\")\n",
        "plt.title(\"Correlation Heatmap of Netflix Data\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      },
      "source": [
        "A heatmap visualizes data values through color intensity, revealing patterns and correlations in a matrix format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfSqtnDqZNRR"
      },
      "source": [
        "The chart shows a moderate negative correlation between release year and duration (-0.24), with other correlations being weak or negligible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q29F0dvdveiT"
      },
      "source": [
        "#### Chart - 10 - Pair Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "outputs": [],
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(df, diag_kind='kde', corner=True)\n",
        "plt.title('Pair Plot of Netflix Dataset')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXh0U9oCveiU"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMmPjTByveiU"
      },
      "source": [
        "A pair plot displays pairwise relationships between multiple variables in a dataset, helping to identify correlations and distributions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22aHeOlLveiV"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPQ8RGwHveiV"
      },
      "source": [
        "* Most content is added in recent years, with a significant spike in additions around 2018-2020.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-ATYxFrGrvw"
      },
      "source": [
        "## ***5. Hypothesis Testing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      },
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yEUt7NnHlrM"
      },
      "source": [
        "### Hypothetical Statement - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI9ZP0laH0D-"
      },
      "source": [
        "Null Hypothesis (H₀): The average duration of movies on Netflix is 90 minutes.\n",
        "Alternate Hypothesis (H₁ or Ha): The average duration of movies on Netflix is not 90 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I79__PHVH19G"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Filter the dataset for movies\n",
        "movies_data = df[df['type'] == 'Movie']\n",
        "\n",
        "# Perform one-sample t-test\n",
        "from scipy.stats import ttest_1samp\n",
        "\n",
        "# Null hypothesis value\n",
        "hypothesized_mean = 90\n",
        "\n",
        "# Calculate the t-test\n",
        "t_stat, p_value = ttest_1samp(movies_data['duration_num'].dropna(), hypothesized_mean)\n",
        "\n",
        "t_stat, p_value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou-I18pAyIpj"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2U0kk00ygSB"
      },
      "source": [
        "One-sample t-test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF3858GYyt-u"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO4K0gP5y3B4"
      },
      "source": [
        "This test compares the mean of a single sample to a known value (90 minutes in this case)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_0_7-oCpUZd"
      },
      "source": [
        "### Hypothetical Statement - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwyV_J3ipUZe"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      },
      "source": [
        "Null Hypothesis (H0): The proportion of TV shows rated TV-MA is equal to the proportion of movies rated TV-MA.\n",
        "Alternate Hypothesis (H1): The proportion of TV shows rated TV-MA is different from the proportion of movies rated TV-MA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yB-zSqbpUZe"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Create a contingency table\n",
        "contingency_table = pd.crosstab(df['type'], df['rating'] == 'TV-MA')\n",
        "\n",
        "# Perform chi-square test\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "chi2, p_value, _, _ = chi2_contingency(contingency_table) # _ , _ because we donot need dof, expected\n",
        "\n",
        "chi2, p_value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUvejAfpUZe"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLDrPz7HpUZf"
      },
      "source": [
        " Chi-square test for independence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd15vwWVpUZf"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xOGYyiBpUZf"
      },
      "source": [
        " This test is appropriate for comparing proportions in categorical data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn_IUdTipZyH"
      },
      "source": [
        "### Hypothetical Statement - 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49K5P_iCpZyH"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gWI5rT9pZyH"
      },
      "source": [
        "Null Hypothesis (H0): There is no difference in the average number of seasons between TV shows added before 2015 and those added after 2015.\n",
        "Alternate Hypothesis (H1): There is a difference in the average number of seasons between TV shows added before 2015 and those added after 2015."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nff-vKELpZyI"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Filter the dataset for TV shows\n",
        "tv_shows_data = df[df['type'] == 'TV Show']\n",
        "\n",
        "# Create two groups based on the year_added\n",
        "group1 = tv_shows_data[tv_shows_data['year_added'] < 2015]['duration_num'].dropna()\n",
        "group2 = tv_shows_data[tv_shows_data['year_added'] >= 2015]['duration_num'].dropna()\n",
        "\n",
        "# Perform independent samples t-test\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "t_stat, p_value = ttest_ind(group1, group2, equal_var=False)\n",
        "\n",
        "t_stat, p_value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLW572S8pZyI"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytWJ8v15pZyI"
      },
      "source": [
        "Independent samples t-test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWbDXHzopZyI"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M99G98V6pZyI"
      },
      "source": [
        "This test compares the means of two independent groups to determine if there is a significant difference between them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t-Y4Zw_xSMN"
      },
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-2bywo6xSMg"
      },
      "source": [
        "### 1. Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwlQqG7vxSMh"
      },
      "outputs": [],
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df_copy = df.copy()\n",
        "df_copy['director'].fillna('Unknown', inplace=True)\n",
        "df_copy['cast'].fillna('Unknown', inplace=True)\n",
        "df_copy['country'].fillna('Unknown', inplace=True)\n",
        "df_copy['date_added'].fillna('Unknown', inplace=True)\n",
        "df_copy['rating'].fillna(df_copy['rating'].mode()[0], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A-kqL7bxSMh"
      },
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJC02BezxSMi"
      },
      "source": [
        "Fill Missing Values with 'Unknown': For categorical columns such as director, cast, and country, filling with 'Unknown' ensures that no data is lost and these entries can still be analyzed.\n",
        "Date Imputation: For date_added, missing values were filled with 'Unknown' and later converted to datetime. If a more accurate date is needed, missing values can be filled using forward/backward fill based on the context.\n",
        "Mode Imputation: For rating, filling missing values with the most frequent value ensures consistency in analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B3qHuWVxSMi"
      },
      "source": [
        "### 2. Handling Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iKLe7bixSMi"
      },
      "outputs": [],
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "from scipy import stats\n",
        "\n",
        "# Detect outliers\n",
        "z_scores = stats.zscore(df_copy['duration_num'].dropna())\n",
        "abs_z_scores = np.abs(z_scores)\n",
        "filtered_entries = (abs_z_scores < 3).all(axis=0)\n",
        "\n",
        "# Capping outliers\n",
        "df_copy.loc[~filtered_entries, 'duration_num'] = df_copy['duration_num'].quantile(0.99)\n",
        "df_copy.loc[df_copy['duration_num'] < df_copy['duration_num'].quantile(0.01), 'duration_num'] = df_copy['duration_num'].quantile(0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9zQjuC7xSMj"
      },
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bef8kVDxSMj"
      },
      "source": [
        "Z-Score Method: For numerical columns like duration_num, outliers are detected using the z-score method and treated by capping them at the 1st and 99th percentiles to reduce their impact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcckdkVbxSMk"
      },
      "source": [
        "### 3. Categorical Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMCnaWPtxSMk"
      },
      "outputs": [],
      "source": [
        "# Encode your categorical columns\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# Label Encoding\n",
        "le = LabelEncoder()\n",
        "df_copy['type'] = le.fit_transform(df_copy['type'])\n",
        "\n",
        "# One-Hot Encoding\n",
        "data = pd.get_dummies(df_copy, columns=['rating', 'country'], drop_first=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-xEFuToxSMk"
      },
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2aKZoQRxSMl"
      },
      "source": [
        "Label Encoding: For binary categorical columns such as type.\n",
        "One-Hot Encoding: For multi-category columns such as rating and country to avoid ordinal relationships."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCfQ-VhzxSMl"
      },
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKGCkyN6xSMl"
      },
      "source": [
        "#### 1. Expand Contraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Nnz2C01xSMm"
      },
      "outputs": [],
      "source": [
        "# Expand Contraction\n",
        "# Combining the textual columns and assigning it to a new variable\n",
        "df_copy['combined_text'] = (df_copy['cast']+' '+df_copy['listed_in']+' '+' '+df_copy['description'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5AGSEc5xSMm"
      },
      "source": [
        "#### 2. Lower Casing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJXM5jS5xSMm"
      },
      "outputs": [],
      "source": [
        "# Function to convert text to lowercase\n",
        "def convert_to_lowercase(text):\n",
        "    # Check if the text is a string before applying lower()\n",
        "    if isinstance(text, str):\n",
        "        return text.lower()\n",
        "    else:\n",
        "        # Handle non-string values (e.g., float)\n",
        "        return str(text).lower()  # Convert to string and then apply lower()\n",
        "\n",
        "df_copy['combined_text'] = df_copy['combined_text'].apply(convert_to_lowercase)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gfiC8Q6xSMn"
      },
      "source": [
        "#### 3. Removing Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DQ-sJr1xSMn"
      },
      "outputs": [],
      "source": [
        "# Function to remove punctuations\n",
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    # Check if the text is a string, if not convert to string\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    return ''.join([char for char in text if char not in string.punctuation])\n",
        "\n",
        "df_copy['combined_text'] = df_copy['combined_text'].apply(remove_punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw2iC9A4xSMo"
      },
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Orfm5ezhxSMo"
      },
      "outputs": [],
      "source": [
        "# Function to remove digits from the text\n",
        "def remove_digits(text):\n",
        "    return ''.join([char for char in text if not char.isdigit()])\n",
        "\n",
        "df_copy['combined_text'] = df_copy['combined_text'].apply(remove_digits)\n",
        "\n",
        "# Function to remove URLs\n",
        "def remove_urls(text):\n",
        "    return ' '.join([word for word in text.split() if 'http' not in word and 'www' not in word])\n",
        "\n",
        "df_copy['combined_text'] = df_copy['combined_text'].apply(remove_urls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uut_TWUqxSMp"
      },
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lxuu4lDqxSMq"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    stop_words = nltk.corpus.stopwords.words('english')\n",
        "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "# Apply stopword removal\n",
        "df_copy['combined_text'] = df_copy['combined_text'].apply(remove_stopwords)\n",
        "\n",
        "# Function to remove extra whitespace\n",
        "def remove_extra_whitespace(text):\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "df_copy['combined_text'] = df_copy['combined_text'].apply(remove_extra_whitespace)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiTigM4axSMr"
      },
      "source": [
        "#### 6. Tokenization (Vectorization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPy59qHsxSMs"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Function to vectorize text\n",
        "def vectorize_text(df_copy):\n",
        "    vectorizer = CountVectorizer()\n",
        "    vectorizer.fit_transform(df_copy['combined_text'])\n",
        "    return vectorizer.vocabulary_\n",
        "\n",
        "vectorized_dict = vectorize_text(df_copy)\n",
        "print('Total Features:', len(vectorized_dict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kbr6DhixSMs"
      },
      "source": [
        "#### 7. Text Normalization (Stemming)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FOxXfM9xSMs"
      },
      "outputs": [],
      "source": [
        "# Normalizing Text\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Function for stemming text\n",
        "def apply_stemming(text):\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    return ' '.join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "df_copy['combined_text'] = df_copy['combined_text'].apply(apply_stemming)\n",
        "\n",
        "# Re-run vectorization after stemming\n",
        "vectorized_dict_after_stem = vectorize_text(df_copy)\n",
        "print('Total Features After Stemming:', len(vectorized_dict_after_stem))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyT6110WxSMt"
      },
      "source": [
        "##### Which text normalization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGha4JlzxSMt"
      },
      "source": [
        "I used Stemming, it removes endings from words, like changing 'coming' to 'come'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQHsd7PfxSMu"
      },
      "source": [
        "#### 8. Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WAtbhg-xSMu"
      },
      "outputs": [],
      "source": [
        "# Vectorizing Text\n",
        "# Function to get number from proportion\n",
        "def proportionToNumber(proportion, data):\n",
        "    return (proportion * data) / 100\n",
        "\n",
        "num = int(round(proportionToNumber(1, len(df_copy)), 0))\n",
        "# Vectorizing Text - discarding features which are available less than 1% of the dataset and more than 90%.\n",
        "count_vectorizer = CountVectorizer(min_df=num, max_df=0.9)\n",
        "features_array = count_vectorizer.fit_transform(df_copy['combined_text']).toarray()\n",
        "features_names = count_vectorizer.get_feature_names_out()\n",
        "df_copy.shape\n",
        "\n",
        "\n",
        "# Making dataframe with vocabularies\n",
        "X = pd.DataFrame(features_array, columns=features_names)\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epbyTfFIxSMu"
      },
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyaxKAOjxSMu"
      },
      "source": [
        "I used CountVectorizer technique,because it counts word frequencies directly, which is sufficient for this task where raw counts matter more than adjusting for document frequency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoQLJGVGxSMv"
      },
      "source": [
        "### 5. Dimesionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jScX-8gixSMv"
      },
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG7DtPeOxSMv"
      },
      "source": [
        "Since there are many features with mostly zero values, we need to reduce the number of features to make the model more efficient.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Atn_tbSfxSMv"
      },
      "outputs": [],
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "# Applying PCA for dimensionality reduction\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Reducing dimensions while retaining 97.5% variance\n",
        "pca = PCA(n_components=0.975)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "\n",
        "# Checking the shape of the transformed data\n",
        "len(X_reduced[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhfwoWOrxSMw"
      },
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rrh4xsepxSMw"
      },
      "source": [
        "I used principal component analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfCC591jGiD4"
      },
      "source": [
        "## ***7. ML Model Implementation***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      },
      "source": [
        "### ML Model - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDtEgl80bwMl"
      },
      "outputs": [],
      "source": [
        "# Function to display clusters\n",
        "def plotClusters(data,cluster_labels,cluster_centers):\n",
        "    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=cluster_labels, s=10, cmap='viridis')\n",
        "    plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', s=15)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "# ML Model - 1 Implementation - KElbowVisualization\n",
        "plt.rcParams['figure.figsize'] = (12,7)\n",
        "model = KMeans(random_state=10)\n",
        "visualizer = KElbowVisualizer(model,k=(2,15),metric='calinski_harabasz',timings=False,locate_elbow=False)\n",
        "# Fit the Algorithm\n",
        "visualizer.fit(X_reduced)\n",
        "# Predict on the model\n",
        "visualizer.show()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5YOPUANcAsd"
      },
      "outputs": [],
      "source": [
        "# Silhouette Score for each cluster\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "clusters_range = np.arange(2, 15)\n",
        "\n",
        "for k in clusters_range:\n",
        "    model = KMeans(n_clusters=k, random_state=42)\n",
        "    predictions = model.fit_predict(X_reduced)\n",
        "    centroids = model.cluster_centers_\n",
        "    silhouette = silhouette_score(X_reduced, predictions)\n",
        "    inertia_metric = model.inertia_\n",
        "    print(f'For {k} clusters, the silhouette score is {silhouette}')\n",
        "\n",
        "    # Plotting the results:\n",
        "    plotClusters(X_reduced, predictions, centroids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mR35sweCcBkN"
      },
      "outputs": [],
      "source": [
        "# Silhouette score is good at 6 clusters therefore I am taking n_clusters = 6.\n",
        "kmeans= KMeans(n_clusters=6, random_state=10,max_iter=100)\n",
        "kmeans.fit(X_reduced)\n",
        "\n",
        "# predict the labels of clusters.\n",
        "labels = kmeans.fit_predict(X_reduced)\n",
        "centers = kmeans.cluster_centers_\n",
        "# plotting the results:\n",
        "plotClusters(X_reduced,labels,centers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArJBuiUVfxKd"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbXk9lzAcl7v"
      },
      "source": [
        "From KElbow visualization, I found that after n_clusters=6, the slope becomes nearly constant, and the silhouette score is relatively high at 0.046"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "clustering_mdl = KMeans()\n",
        "search_params = {'n_clusters':[6],'random_state':[10],'max_iter':[15,20],'tol':[.01,.1]}\n",
        "grid_search = GridSearchCV(clustering_mdl, param_grid=search_params, verbose=2, cv=2)\n",
        "# Fit the Algorithm\n",
        "grid_search.fit(X_reduced)\n",
        "# Predict on the model\n",
        "best_model = grid_search.best_estimator_\n",
        "cluster_labels = best_model.predict(X_reduced)\n",
        "print(grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t740a8e4cujO"
      },
      "outputs": [],
      "source": [
        "# Plotting the results:\n",
        "cluster_centroids = best_model.cluster_centers_\n",
        "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=cluster_labels, s=10, cmap='viridis')\n",
        "plt.scatter(cluster_centroids[:, 0], cluster_centroids[:, 1], c='red', s=15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "negyGRa7fxKf"
      },
      "source": [
        "I think cross validation is not needed for clustering. I used n_clusters and max_iter hyper parameters to tune the parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfvqoZmBfxKf"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaLui8CcfxKf"
      },
      "source": [
        "I haven't found any improvement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      },
      "source": [
        "### ML Model - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import scipy.cluster.hierarchy as sci\n",
        "#finding the optimal number of clusters and largest vertical distance without crossing any other horizontal line using the dendrogram\n",
        "hierarchical_linkage = sci.linkage(X_reduced, method='ward')\n",
        "dendrogram = sci.dendrogram(hierarchical_linkage)\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Data Points')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "# Fitting hierarchical clustering to the dataset\n",
        "# 'affinity' is not needed when using 'ward' linkage.\n",
        "hierarchical_clustering = AgglomerativeClustering(n_clusters=4, linkage='ward')\n",
        "cluster_labels_hc = hierarchical_clustering.fit_predict(X_reduced)\n",
        "\n",
        "# Plotting the results:\n",
        "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=cluster_labels_hc, s=10, cmap='Paired')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MsT63u5FNWW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWYfwnehpsJ1"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hierarchical clustering algorithm was used. This unsupervised machine learning model groups data points into a tree-like structure based on their similarity.Due to the presence of noise in the dataset, the results are not ideal. However, for n_clusters = 4, the clustering appears reasonable and better than other configurations."
      ],
      "metadata": {
        "id": "3llQDUJcQRHS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fze-IPXLpx6K"
      },
      "source": [
        "### ML Model - 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "# ML Model - 3 Implementation\n",
        "dbscan = DBSCAN(eps=.91,min_samples=15,algorithm='auto')\n",
        "# Fit the Algorithm\n",
        "dbscan.fit(X_reduced)\n",
        "# Predict on the model\n",
        "predicted_labels = dbscan.fit_predict(X_reduced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AN1z2sKpx6M"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.scatter(X_reduced[:,0], X_reduced[:,1], c=predicted_labels,s=5,cmap='coolwarm')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DBSCAN, a density-based clustering model, was used. It struggled with high noise, forming one large cluster and a few small ones, making it less effective for this dataset."
      ],
      "metadata": {
        "id": "dYKJhCWlJDs1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_CCil-SKHpo"
      },
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHVz9hHDKFms"
      },
      "source": [
        "Euclidean distance was chosen to measure the closeness of points in a multidimensional space, critical for clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBFFvTBNJzUa"
      },
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      },
      "source": [
        "KMeans is a good choice for its speed and better clustering results, outperforming DBSCAN in handling noise and providing meaningful clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCX9965dhzqZ"
      },
      "source": [
        "# **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      },
      "source": [
        "* Dataset Overview:\n",
        "\n",
        "  * The dataset consists of 7787 rows and 12 columns.\n",
        "* Missing Values:\n",
        "\n",
        "  * Director column: 2389 missing values (30.68% of entries)\n",
        "  * Cast column: 718 missing values (9.22%)\n",
        "  * Country column: 507 missing values (6.51%)\n",
        "  * Date_added column: Only 10 missing values (0.13%)\n",
        "* Feature Engineering:\n",
        "\n",
        "  * After vectorization, the dataset was expanded to 46,370 features.\n",
        "  * Post stemming, the number of features was reduced to 39,864.\n",
        "* Dimensionality Reduction:\n",
        "\n",
        "  * After applying dimensionality reduction techniques, the feature set was further condensed to 353 features.\n",
        "* Cluster Analysis:\n",
        "\n",
        "  * Based on the KElbow visualization, the optimal number of clusters is 6, where the curve flattens, indicating minimal benefit from increasing the number of clusters.\n",
        "  * The Silhouette score at n_clusters = 6 is 0.046, suggesting a reasonable level of cluster cohesion and separation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIfDvo9L0UH2"
      },
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}